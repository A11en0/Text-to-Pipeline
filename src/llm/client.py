"""
LLM client, responsible for interacting with the large language model API
Supports retry functionality
"""
import os
import time
import random
import requests
import json
import logging
from typing import Any, Dict, Optional, Union
from src.config import LLM_CONFIG

# Configure logging
logger = logging.getLogger(__name__)

class LLMClient:
    """
    LLM client class, handles interaction with the large language model API, supports automatic retries
    """
    
    def __init__(self, config=None, model_name=None, 
                 max_retries=3, retry_delay=1, retry_backoff=2, 
                 retry_jitter=0.1, retry_codes=None):
        """
        Initialize the LLM client
        
        Args:
            config: LLM configuration dictionary, if not provided, the global configuration will be used
            model_name: The name of the model to use, if provided, the corresponding model configuration will be fetched from the configuration
            max_retries: Maximum number of retries
            retry_delay: Initial retry delay (seconds)
            retry_backoff: Retry backoff multiplier
            retry_jitter: Random jitter ratio to prevent multiple clients from retrying simultaneously
            retry_codes: List of HTTP status codes that require retries, defaults to [429, 500, 502, 503, 504]
        """
        # If no configuration is provided, use the global configuration
        if config is None:
            config = LLM_CONFIG
        
        # If no model name is specified but a default model is set in the configuration, use the default model
        if model_name is None and "default_model" in config:
            model_name = config["default_model"]
            
        # If a model name is specified and there is a models configuration in the config
        if model_name is not None and "models" in config and model_name in config["models"]:
            # Fetch the specific model's configuration
            model_config = config["models"][model_name]
            
            # Merge global configuration and model-specific configuration
            # Only copy global configurations that are not in model_config
            for key, value in config.items():
                if key != "models" and key != "default_model" and key not in model_config:
                    model_config[key] = value
                    
            # Use the merged model configuration
            self.model = model_config.get("model", model_name)
            self.api_key = model_config.get("api_key", os.getenv("OPENAI_API_KEY"))
            self.temperature = model_config.get("temperature", 0.2)
            self.max_tokens = model_config.get("max_tokens", 4000)
            self.api_base = model_config.get("api_base", "https://api.openai.com/v1")
            self.is_reasoning = model_config.get("is_reasoning", False)
        else:
            # Use the provided configuration or global configuration (for backward compatibility)
            self.model = config.get("model", "gpt-4o-mini")
            self.api_key = config.get("api_key", os.getenv("OPENAI_API_KEY"))
            self.temperature = config.get("temperature", 0.2)
            self.max_tokens = config.get("max_tokens", 4000)
            self.api_base = config.get("api_base", "https://api.openai.com/v1")
            self.is_reasoning = config.get("is_reasoning", False)
        
        if not self.api_key:
            raise ValueError("An API key must be provided (via configuration or environment variable OPENAI_API_KEY)")
            
        # Retry configuration
        self.max_retries = max_retries
        self.retry_delay = retry_delay
        self.retry_backoff = retry_backoff
        self.retry_jitter = retry_jitter
        self.retry_codes = retry_codes or [400, 401, 402, 403, 404, 429, 500, 502, 503, 504]  # Default status codes for retries
    
    def generate(self, prompt):
        """
        Generate a text response, supports retries
        
        Args:
            prompt: Prompt text
            
        Returns:
            str or dict: The response text generated by the model or a dictionary containing content and reasoning content
        """
        return self._retry_with_backoff(self._generate, prompt)
    
    def _generate(self, prompt):
        """
        Actual generation logic, wrapped by retry
        """
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        data = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}]
        }
        
        if self.model == "o3-mini" or self.model == "o4-mini":
            data["max_completion_tokens"] = self.max_tokens  # o3-mini uses max_completion_tokens
        else:
            data["max_tokens"] = self.max_tokens  # Other models use max_tokens
            data["temperature"] = self.temperature
            
        response = requests.post(
            f"{self.api_base}/chat/completions",
            headers=headers,
            data=json.dumps(data)
        )
        
        # If the response is unsuccessful, raise an exception to trigger the retry mechanism
        response.raise_for_status()
        
        result = response.json()
        
        # Handle response formats for different models
        if self.is_reasoning and "reasoning_content" in result["choices"][0]["message"]:
            return {
                "content": result["choices"][0]["message"]["content"],
                "reasoning_content": result["choices"][0]["message"]["reasoning_content"]
            }
        else:
            return result["choices"][0]["message"]["content"]
    
    def _retry_with_backoff(self, func, *args, **kwargs):
        """
        Retry using an exponential backoff strategy
        
        Args:
            func: The function to retry
            *args, **kwargs: Function arguments
            
        Returns:
            The return value of the function
            
        Raises:
            Exception: When retries are exhausted and the function still fails
        """
        retries = 0
        while True:
            try:
                return func(*args, **kwargs)
                
            except requests.exceptions.HTTPError as e:
                # Check if the status code requires a retry
                if hasattr(e.response, 'status_code') and e.response.status_code in self.retry_codes:
                    if retries >= self.max_retries:
                        logger.error(f"Maximum retry attempts reached ({self.max_retries}), request failed: {e}")
                        raise Exception(f"LLM API call failed (retried {retries} times): {str(e)}")
                    
                    retries += 1
                    wait_time = self._calculate_wait_time(retries)
                    logger.warning(f"API request failed (status code: {e.response.status_code}), retrying {retries} time(s), waiting {wait_time:.2f} seconds...")
                    time.sleep(wait_time)
                else:
                    # Do not retry for other HTTP errors
                    logger.error(f"API request failed (status code: {e.response.status_code}): {e}")
                    raise Exception(f"LLM API call failed: {str(e)}")
                    
            except requests.exceptions.RequestException as e:
                # Network connection errors can be retried
                if retries >= self.max_retries:
                    logger.error(f"Maximum retry attempts reached ({self.max_retries}), network request failed: {e}")
                    raise Exception(f"LLM API network connection failed (retried {retries} times): {str(e)}")
                
                retries += 1
                wait_time = self._calculate_wait_time(retries)
                logger.warning(f"Network connection failed, retrying {retries} time(s), waiting {wait_time:.2f} seconds... Error: {str(e)}")
                time.sleep(wait_time)
            except KeyError as e:
                if retries >= self.max_retries:
                    logger.error(f"Maximum retry attempts reached ({self.max_retries}), network request failed: {e}")
                    raise Exception(f"LLM API network connection failed (retried {retries} times): {str(e)}")
                retries += 1
                time.sleep(20)
            except Exception as e:
                # Do not retry for other unknown errors
                logger.error(f"An unknown error occurred while calling the LLM API: {e}")
                raise Exception(f"LLM API call failed: {str(e)}")
    
    def _calculate_wait_time(self, retry_number):
        """
        Calculate the wait time for the next retry using an exponential backoff strategy with random jitter
        
        Args:
            retry_number: Current retry count
            
        Returns:
            float: Wait time (seconds)
        """
        # Base wait time, exponentially increasing
        wait_time = self.retry_delay * (self.retry_backoff ** (retry_number - 1))
        
        # Add random jitter
        jitter = random.uniform(-self.retry_jitter, self.retry_jitter)
        wait_time = wait_time * (1 + jitter)
        
        return wait_time